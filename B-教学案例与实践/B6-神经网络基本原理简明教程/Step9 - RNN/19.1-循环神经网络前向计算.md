<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 初步认识RNN

### 提出问题

我们先用一个最简单的序列问题来了解一下RNN的基本运作方式。

假设有一个随机信号发射器，每秒产生一个随机信号，随机值为(0,1)之间。信号发出后，碰到一面墙壁反射回来，来回的时间相加正好是1秒，于是接收器就收到了1秒钟之前的信号。对于接收端来说，可以把接收到的数据序列列表如下：

|时刻|t1|t2|t3|t4|t5|t6|...|
|---|---|---|---|---|---|---|---|
|发射随机信号X|0.35|0.46|0.12|0.69|0.24|0.94|...|
|接收回波信号Y|0|0.35|0.46|0.12|0.69|0.24|...|

具体的描述此问题：当接收端接收到两个连续的值，如0.35、0.46时，系统响应为0.35；下一个时间点接收到了0.12，考虑到上一个时间点的0.46，则二者组合成0.46、0.12序列，此时系统响应为0.46；依此类推，即接收到第二个数值时，总要返回相邻的第一个数值。

我们可以把发射信号看作X，把接收信号看作是Y，则此问题变成了给定样本X和标签值Y，训练一个神经网络，令其当接收到两个序列的值时，总返回第一个值。

读者可能会产生疑问：这个问题用一个最简单不过的程序就可以解决，我们为什么还要大动干戈地使用神经网络呢？如：

```Python
def echo(x1,x2):
    return x2
```

因为这是一个最基本的序列问题，我们先用它投石问路，逐步地理解RNN的精髓所在。

如果把发射信号和回波信号绘制成图，如下图所示：

|样本图|局部放大图|
|---|---|
|<img src="../Images/19/random_number_echo_data.png" ch="500" />|<img src="../Images/19/random_number_echo_data_zoom.png" ch="500" />|
图一：信号及回波样本序列

其中，红色叉子为样本数据点，蓝色圆点为标签数据点，它总是落后于样本数据一个时间步。还可以看到以上数据形成的曲线完全随机，毫无规律。

我们回忆一下，在验证万能近似定理时，我们学习了曲线拟合问题，即带有一个隐层和非线性激活函数的前馈神经网络，可以拟合任意曲线。但是在这个问题里，有几点不同：

1. 不是连续值，而是时间序列的离散值
2. 完全随机的离散值，而不是满足一定的规律
3. 测试数据不在样本序列里，完全独立

所以，即使使用DNN技术中曲线拟合技术得到了一个拟合网络，也不能正确地预测不在样本序列里的测试集数据。但是，我们可以把DNN做一个变形，让它能够处理时间序列数据：

<img src="../Images/19/random_number_echo.png" ch="500" />
图二：两个时间步的DNN

图二中含有两个简单的DNN网络，t1和t2，每个节点上都只有一个神经元，其中，各个节点的名称和含义是：

|名称|含义|在t1,t2上的取值|
|---|---|---|
|x|输入层样本|根据样本值|
|U|x到h的权重值|相同|
|h|隐层|不同|不同|
|bh|h节点的偏移值|相同|
|tanh|激活函数|函数相同|
|s|隐层激活状态|不同|
|V|s到z的权重值|相同|
|z|输出层|不同|
|bz|z的偏移值|相同|
|loss|损失函数|函数相同|
|y|标签值|根据标签值|

由于是一个值拟合的网络，所以在输出层不使用分类函数，损失函数使用 MSE 均方差。在这个具体的问题中，t2的标签值y应该和t1的样本值x相同。

请读者注意，在很多RNN的文字资料中，通常把h和s合并在一起。在这里我们把它们分开画，便于后面的反向传播的推导和理解。

还有一个问题是，为什么t1的后半部分是虚线的？因为在这个问题中，我们只对t2的输出感兴趣，检测t2的输出值z和y的差距是多少，而不关心t1的输出是什么，所以不必计算t1的z值和loss值，处于无监督状态。

t1和t2是两个独立的网络，在t1和t2之间，用一个W连接t1的隐层激活状态值到t2的隐层输入，对t2来说，相当于有两个输入：一个是t2时刻的样本值x，一个是t1时刻的隐层激活值s。所以，它们的前向计算公式为：

对于t1：

$$
h_{t1}=x_{t1} \cdot U + b_h\tag{1}
$$
$$
s_{t1} = tanh(h_{t1}) \tag{2}
$$

对于t2：
$$
h_{t2}=x_{t2} \cdot U + s_{t1} \cdot W +b_h\tag{3}
$$
$$
s_{t2} = tanh(h_{t2}) \tag{4}
$$
$$
z_{t2} = s_{t2} \cdot V + b_z\tag{5}
$$
$$
loss = \frac{1}{2}(z_{t2}-y_{t2})^2 \tag{6}
$$

公式1至公式6中，所有的变量均为标量，这就有利于我们对反向传播的推导，不用考虑矩阵、向量的求导运算。

我们首先对t2网络进行反向传播推导：

$$
\frac{\partial loss}{\partial z_{t2}}=z_{t2}-y_{t2} \rightarrow dz_{t2} \tag{7}
$$
$$
\begin{aligned}
\frac{\partial loss}{\partial h_{t2}}&=\frac{\partial loss}{\partial z_{t2}}\frac{\partial z_{t2}}{\partial s_{t2}}\frac{\partial s_{t2}}{\partial h_{t2}} \\
&=dz_{t2} \cdot V \cdot tanh'(s_{t2}) \\
&=dz_{t2} \cdot V \cdot (1-s_{t2}^2) \rightarrow dh_{t2} \tag{8}
\end{aligned}
$$

$$
\frac{\partial loss}{\partial b_z}=\frac{\partial loss}{\partial z_{t2}}\frac{\partial z_{t2}}{\partial b_z}=dz_{t2} \rightarrow db_{z_{t2}} \tag{9}
$$

$$
\frac{\partial loss}{\partial b_h}=\frac{\partial loss}{\partial h_{t2}}\frac{\partial h_{t2}}{\partial b_h}=dh_{t2} \rightarrow db_{h_{t2}} \tag{10}
$$

$$
\frac{\partial loss}{\partial V}=\frac{\partial loss}{\partial z_{t2}}\frac{\partial z_{t2}}{\partial V}=dz_{t2} \cdot s \rightarrow dV_{t1}  \tag{11}
$$

$$
\frac{\partial loss}{\partial U}=\frac{\partial loss}{\partial h_{t2}}\frac{\partial h_{t2}}{\partial U}=dh_{t2} \cdot x_{t2} \rightarrow dU_{t1} \tag{12}
$$

$$
\frac{\partial loss}{\partial W}=\frac{\partial loss}{\partial h_{t2}}\frac{\partial h_{t2}}{\partial W}=dh_{t2} \cdot s_{t1} \rightarrow dW_{t2} \tag{13}
$$

下面我们对t1网络进行反向传播推导。由于t1的后半部分输出是没有监督的，所以我们不必考虑后半部分的反向传播问题，只从s节点开始向后计算。

$$
\frac{\partial loss}{\partial h_{t1}}=\frac{\partial loss}{\partial h_{t2}}\frac{\partial h_{t2}}{\partial s_{t1}}\frac{\partial s_{t1}}{\partial h_{t1}}=dh_{t2} \cdot W \cdot (1-s_{t1}^2) \rightarrow dh_{t1} \tag{14}
$$

$$
\frac{\partial loss}{\partial b_h}=\frac{\partial loss}{\partial h_{t1}}\frac{\partial h_{t1}}{\partial b_h}=dh_{t1} \rightarrow db_{h_{t1}} \tag{15}
$$
$$
db_{z_{t1}} = 0 \tag{16}
$$
$$
\frac{\partial loss}{\partial U}=\frac{\partial loss}{\partial h_{t1}}\frac{\partial h_{t1}}{\partial U}=dh_{t1} \cdot x_{t1} \rightarrow dU_{t1} \tag{17}
$$
$$
dV_{t1} = 0 \tag{18}
$$
$$
dW_{t1}=0 \tag{19}
$$
