<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 19.2 进一步认识RNN


### 19.2.1 提出问题

在加减法运算中，总会遇到进位或者退位的问题，我们以二进制为例，比如13-5=8这个十进制的减法，变成二进制后如下所示：

```
13 - 6 = 7
====================
  x1: [1, 1, 0, 1]
- x2: [0, 1, 1, 0]
------------------
  y:  [0, 1, 1, 1]
====================
```
- 被减数13变成了[1, 1, 0, 1]
- 减数6变成了[0, 1, 1, 0]
- 结果的7变成了[0, 1, 1, 1]

在减法过程中：
- x1和x2的最后一位是1和0，相减为1
- 倒数第二位是0和1，需要从前面借一位，相减后得1
- 倒数第三位本来是1和1，借位后变成了0和1，再从前面借一位，相减后得1
- 倒数第四位现在是0和0，相减为0

也就是说，在减法过程中，后面的计算会影响前面的值，所以必须逐位计算，这也就是序列的概念，所以应该可以用RNN的技术来解决。

由于计算是从最后一位开始的，我们认为最后一位是第一个时间步，所以需要把样本数据的前后顺序颠倒一下，比如13，从二进制的 [1, 1, 0, 1] 倒序变成 [1, 0, 1, 1]。相应地，标签数据7也要从二进制的 [0, 1, 1, 1] 倒序变成 [1, 1, 1, 0]。

在这里个例子中，因为是4位二进制减法，所以最大值是15，即 [1, 1, 1, 1]；最小值是0，并且要求被减数必须大于减数，所以样本的数量一共是136个，每个样本含有两组4位的二进制数，表示被减数和减数。标签值为一组4位二进制数。三组二进制数都是倒序。

### 19.2.2 组建多个时序的网络

#### 搭建网络

在本例中，我们仍然从DNN的结构扩展到含有4个时序的网络结构：

<img src="../Images/19/binary_number_minus_net.png" width="600" />

上图中，最左侧的简易结构是通常的RNN的画法，而右侧是其展开后的细节，由此可见细节有很多，如果不展开的话，对于初学者来说很难理解，而且也不利于我们进行反向传播的推导。

再重复一下，请读者记住，t1是二进制数的最低位，但是由于我们把样本倒序了，所以，现在的t1就是样本的第0个单元的值。并且由于涉及到被减数和减数，所以每个样本的第0个单元（时间步）都有两个特征值，其它3个单元也一样。

在上图中，连接x和h的是一条线标记为U，在19.1节的例子中，U是一个数，但是在本节中，U是一个 1x4 的参数矩阵，V是一个 4x1 的参数矩阵，而W就是一个 4x4 的参数矩阵。我们把它们展开画成下图（其中把s和h合并在一起了）：

<img src="../Images/19/binary_number_minus_unfold.png" width="600" />

U和V都比较容易理解，而W是一个连接相邻时序的参数矩阵，并且共享相同的参数值，这一点在刚开始接触RNN时不太容易理解。上图中把W绘制成3种颜色，是想让读者看得清楚些，并不代表它们是不同的值。

与19.1节不同的是，在每个时间步的结构中，多出来一个a，是从z经过Logistic函数生成的。这是为什么呢？因为在本例中，我们想模拟二进制数的减法，所以结果应该是0或1，于是我们把它看作是二分类问题，z的值是一个浮点数，而a的值尽量向0或1靠近，所以用Logistic作为二分类函数。

二分类问题的损失函数使用交叉熵函数，这与我们在DNN中学习的二分类问题完全相同。

#### 正向计算

下面我们先看看4个时序的正向计算过程。

从图中看，t2、t3、t4的结构是一样的，只有t1缺少了从前面的时间步的输入，因为它是第一个时序，前面没有输入，所以我们单独定义t1的前向计算函数：

$$
h = x \cdot U \tag{1}
$$

$$
s = tanh(h) \tag{2}
$$

$$
z = s \cdot V \tag{3}
$$

$$
a = Logistic(z) \tag{4}
$$

$$
loss = -[y \ln a + (1-y) \ln (1-a)] \tag{5}
$$

细心的读者可能会注意到在公式1和公式3中，我们并没有添加偏移项b，是因为在此问题中，没有偏移项一样可以完成任务。

```Python
class timestep_1(timestep):
    # compare with timestep class: no h_t value from previous layer
    def forward(self,x,U,V,W):
        self.U = U
        self.V = V
        self.W = W
        self.x = x
        # 公式1
        self.h = np.dot(self.x, U)
        # 公式2
        self.s = Tanh().forward(self.h)
        # 公式3
        self.z = np.dot(self.s, V)
        # 公式4
        self.a = Logistic().forward(self.z)
```        

其它三个时间步的前向计算过程是一样的，它们与t1的不同之处在于公式1，所以我们单独说明一下：

$$
h = x \cdot U + s_{t-1} \cdot W \tag{6}
$$

```Python
class timestep(object):
    def forward(self,x,U,V,W,prev_s):
        ...
        # 公式6
        self.h = np.dot(x, U) + np.dot(prev_s, W)
        ...
```

#### 反向传播

反向传播的计算对于4个时间步来说，分为3种过程，但是它们之间只有微小的区别。我们先把公共的部分列出来，再说明每个时间步的差异。

首先是损失函数对z的偏导数：

$$
\begin{aligned}
\frac{\partial loss}{\partial z}&=\frac{\partial loss}{\partial a}\frac{\partial a}{\partial z} \\
&=\frac{a-y}{a(1-a)}a(1-a) \\
&= a-y \rightarrow dz \tag{7}
\end{aligned}
$$

再进一步计算s的误差。对于t4来说，s节点的路径比较单一，直接从loss节点向下反向推导即可：

$$
\frac{\partial loss}{\partial s}=\frac{\partial loss}{\partial z}\frac{\partial z}{\partial s} = dz \cdot V^T \tag{8}
$$

$$
\frac{\partial loss}{\partial h}=\frac{\partial loss}{\partial s}\frac{\partial s}{\partial h}=dz \cdot V^T \odot (1-s^2) \rightarrow dh_{t4} \tag{9}
$$

对于t1、t2、t3的s节点来说，都有两个方向的反向路径，第一个是从本时间步的z节点，第二个是从后一个时间步的h节点，因此，s的反向计算应该是两个路径的和：

$$
\begin{aligned}
\frac{\partial J}{\partial s_t}&=\frac{\partial loss_t}{\partial s_t} + \frac{\partial loss_{t+1}}{\partial s_t} \\
&=\frac{\partial loss_t}{\partial s_t} + \frac{\partial loss_{t+1}}{\partial h_{t+1}}\frac{\partial h_{t+1}}{\partial s_t} \\
&=dz \cdot V^T + dh_{t+1} \cdot W^T \tag{10}
\end{aligned}
$$

再进一步计算t1到t3的h节点的误差：

$$
\frac{\partial J}{\partial h_t} = \frac{\partial J}{\partial s_t} \frac{\partial s_t}{\partial h_t} = (dz \cdot V^T + dh_{t+1} \cdot W^T) \odot (1-s_t^2) \tag{11}

$$





对于t1来说，它的前面没有关于W的输入，所以不需要计算W的误差：

$$

$$


下面是最后几轮的打印输出结果：

```
...
11 1525
loss=0.087381, acc=0.977941
...
loss=0.081522, acc=1.000000
testing...
loss=0.080641, acc=1.000000
```

下面随机列出了几个测试样本及其预测结果：

```
  x1: [1, 0, 1, 1]
- x2: [0, 0, 0, 1]
------------------
true: [1, 0, 1, 0]
pred: [1, 0, 1, 0]
11 - 1 = 10
====================

  x1: [1, 1, 1, 1]
- x2: [0, 0, 1, 1]
------------------
true: [1, 1, 0, 0]
pred: [1, 1, 0, 0]
15 - 3 = 12
====================

  x1: [1, 1, 0, 1]
- x2: [0, 1, 1, 0]
------------------
true: [0, 1, 1, 1]
pred: [0, 1, 1, 1]
13 - 5 = 8
====================
```